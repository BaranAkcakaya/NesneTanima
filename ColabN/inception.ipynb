{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "inception.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wlj6z5Mq-HFR",
        "outputId": "ad91f820-24a6-4003-d4f1-145912e38788",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "#%cd /gdrive"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GvukFLt-PRd"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import os\n",
        "import sys\n",
        "import tarfile\n",
        "import zipfile\n",
        "import urllib.request\n",
        "import pickle"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCQU3eb0BWjs",
        "outputId": "95505801-5338-48ed-c17d-89d7cf865ce8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "\n",
        "data_url = \"http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz\"\n",
        "data_path = \"/gdrive/My Drive/incep/data/inception/\"\n",
        "path_uid_to_cls = \"imagenet_2012_challenge_label_map_proto.pbtxt\"\n",
        "path_uid_to_name = \"imagenet_synset_to_human_label_map.txt\"\n",
        "path_graph_def = \"classify_image_graph_def.pb\"\n",
        "\n",
        "def _download_and_extract(url, download_dir):\n",
        "    filename = url.split('/')[-1]\n",
        "    file_path = os.path.join(download_dir, filename)\n",
        "\n",
        "    if not os.path.exists(file_path):\n",
        "        if not os.path.exists(download_dir):\n",
        "            os.makedirs(download_dir)\n",
        "\n",
        "        def progress(block_num, block_size, total_size):\n",
        "            progress_info = [url, float(block_num * block_size) / float(total_size) * 100.0]\n",
        "            print('\\r Downloading {} - {:.2f}%'.format(*progress_info), end=\"\")\n",
        "        file_path, _ = urllib.request.urlretrieve(url=url,\n",
        "                                                  filename=file_path,\n",
        "                                                  reporthook=progress)\n",
        "\n",
        "        print()\n",
        "        print(\"Download finished. Extracting files.\")\n",
        "        if file_path.endswith(\".zip\"):\n",
        "            zipfile.ZipFile(file=file_path, mode=\"r\").extractall(download_dir)\n",
        "        elif file_path.endswith((\".tar.gz\", \".tgz\")):\n",
        "            tarfile.open(name=file_path, mode=\"r:gz\").extractall(download_dir)\n",
        "        print(\"Done.\")\n",
        "    else:\n",
        "        print(\"Data already exists.\")\n",
        "\n",
        "def download():\n",
        "    _download_and_extract(data_url, data_path)\n",
        "\n",
        "\n",
        "def cache(cache_path, fn, *args, **kwargs):\n",
        "    if os.path.exists(cache_path):\n",
        "        with open(cache_path, mode='rb') as file:\n",
        "            obj = pickle.load(file)\n",
        "\n",
        "        print(\"- Data loaded from cache-file: \" + cache_path)\n",
        "    else:\n",
        "        obj = fn(*args, **kwargs)\n",
        "        with open(cache_path, mode='wb') as file:\n",
        "            pickle.dump(obj, file)\n",
        "\n",
        "        print(\"- Data saved to cache-file: \" + cache_path)\n",
        "\n",
        "    return obj\n",
        "\n",
        "\n",
        "class NameLookup:\n",
        "    def __init__(self):\n",
        "        self._uid_to_cls = {}\n",
        "        self._uid_to_name = {}\n",
        "        self._cls_to_uid = {}\n",
        "\n",
        "        path = os.path.join(data_path, path_uid_to_name)\n",
        "        with open(file=path, mode='r') as file:\n",
        "            lines = file.readlines()\n",
        "\n",
        "            for line in lines:\n",
        "                line = line.replace(\"\\n\", \"\")\n",
        "                elements = line.split(\"\\t\")\n",
        "                uid = elements[0]\n",
        "                name = elements[1]\n",
        "                self._uid_to_name[uid] = name\n",
        "\n",
        "        path = os.path.join(data_path, path_uid_to_cls)\n",
        "        with open(file=path, mode='r') as file:\n",
        "            lines = file.readlines()\n",
        "\n",
        "            for line in lines:\n",
        "                if line.startswith(\"  target_class: \"):\n",
        "                    elements = line.split(\": \")\n",
        "                    cls = int(elements[1])\n",
        "\n",
        "                elif line.startswith(\"  target_class_string: \"):\n",
        "                    elements = line.split(\": \")\n",
        "                    uid = elements[1]\n",
        "                    uid = uid[1:-2]\n",
        "                    self._uid_to_cls[uid] = cls\n",
        "                    self._cls_to_uid[cls] = uid\n",
        "\n",
        "    def uid_to_cls(self, uid):\n",
        "        return self._uid_to_cls[uid]\n",
        "\n",
        "    def uid_to_name(self, uid, only_first_name=False):\n",
        "        name = self._uid_to_name[uid]\n",
        "        if only_first_name:\n",
        "            name = name.split(\",\")[0]\n",
        "\n",
        "        return name\n",
        "\n",
        "    def cls_to_name(self, cls, only_first_name=False):\n",
        "        uid = self._cls_to_uid[cls]\n",
        "        name = self.uid_to_name(uid=uid, only_first_name=only_first_name)\n",
        "\n",
        "        return name\n",
        "\n",
        "\n",
        "class Inception:\n",
        "    tensor_name_input_jpeg = \"DecodeJpeg/contents:0\"\n",
        "    tensor_name_input_image = \"DecodeJpeg:0\"\n",
        "    tensor_name_resized_image = \"ResizeBilinear:0\"\n",
        "    tensor_name_softmax = \"softmax:0\"\n",
        "    tensor_name_softmax_logits = \"softmax/logits:0\"\n",
        "    tensor_name_transfer_layer = \"pool_3:0\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.name_lookup = NameLookup()\n",
        "        self.graph = tf.Graph()\n",
        "        with self.graph.as_default():\n",
        "            path = \"/gdrive/My Drive/incep/data/inception/classify_image_graph_def.pb\"\n",
        "            with tf.compat.v1.gfile.FastGFile(path, 'rb') as file:\n",
        "                graph_def = tf.compat.v1.GraphDef()\n",
        "                graph_def.ParseFromString(file.read())\n",
        "                tf.import_graph_def(graph_def, name='')\n",
        "\n",
        "        self.y_pred = self.graph.get_tensor_by_name(self.tensor_name_softmax)\n",
        "        self.y_logits = self.graph.get_tensor_by_name(self.tensor_name_softmax_logits)\n",
        "        self.resized_image = self.graph.get_tensor_by_name(self.tensor_name_resized_image)\n",
        "        self.transfer_layer = self.graph.get_tensor_by_name(self.tensor_name_transfer_layer)\n",
        "        self.transfer_len = self.transfer_layer.get_shape()[3]\n",
        "\n",
        "        self.session = tf.compat.v1.Session(graph=self.graph)\n",
        "\n",
        "    def close(self):\n",
        "        self.session.close()\n",
        "\n",
        "    def _write_summary(self, logdir='summary/'):\n",
        "        writer = tf.train.SummaryWriter(logdir=logdir, graph=self.graph)\n",
        "        writer.close()\n",
        "\n",
        "    def _create_feed_dict(self, image_path=None, image=None):\n",
        "        if image is not None:\n",
        "            feed_dict = {self.tensor_name_input_image: image}\n",
        "\n",
        "        elif image_path is not None:\n",
        "            image_data = tf.compat.v1.gfile.FastGFile(image_path, 'rb').read()\n",
        "\n",
        "            feed_dict = {self.tensor_name_input_jpeg: image_data}\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Either image or image_path must be set.\")\n",
        "\n",
        "        return feed_dict\n",
        "\n",
        "    def classify(self, image_path=None, image=None):\n",
        "        feed_dict = self._create_feed_dict(image_path=image_path, image=image)\n",
        "        pred = self.session.run(self.y_pred, feed_dict=feed_dict)\n",
        "        pred = np.squeeze(pred)\n",
        "\n",
        "        return pred\n",
        "\n",
        "    def get_resized_image(self, image_path=None, image=None):\n",
        "        feed_dict = self._create_feed_dict(image_path=image_path, image=image)\n",
        "        resized_image = self.session.run(self.resized_image, feed_dict=feed_dict)\n",
        "        resized_image = resized_image.squeeze(axis=0)\n",
        "        resized_image = resized_image.astype(float) / 255.0\n",
        "\n",
        "        return resized_image\n",
        "\n",
        "    def print_scores(self, pred, k=10, only_first_name=True):\n",
        "        idx = pred.argsort()\n",
        "        top_k = idx[-k:]\n",
        "        for cls in reversed(top_k):\n",
        "            name = self.name_lookup.cls_to_name(cls=cls, only_first_name=only_first_name)\n",
        "            score = pred[cls]\n",
        "\n",
        "            print(\"{0:>6.2%} : {1}\".format(score, name))\n",
        "\n",
        "    def transfer_values(self, image_path=None, image=None):\n",
        "        feed_dict = self._create_feed_dict(image_path=image_path, image=image)\n",
        "        transfer_values = self.session.run(self.transfer_layer, feed_dict=feed_dict)\n",
        "        transfer_values = np.squeeze(transfer_values)\n",
        "\n",
        "        return transfer_values\n",
        "\n",
        "\n",
        "\n",
        "def process_images(fn, images=None, image_paths=None):\n",
        "    using_images = images is not None\n",
        "    if using_images:\n",
        "        num_images = len(images)\n",
        "    else:\n",
        "        num_images = len(image_paths)\n",
        "    result = [None] * num_images\n",
        "    for i in range(num_images):\n",
        "        msg = \"\\r- Processing image: {0:>6} / {1}\".format(i+1, num_images)\n",
        "        sys.stdout.write(msg)\n",
        "        sys.stdout.flush()\n",
        "        if using_images:\n",
        "            result[i] = fn(image=images[i])\n",
        "        else:\n",
        "            result[i] = fn(image_path=image_paths[i])\n",
        "\n",
        "    print()\n",
        "\n",
        "    result = np.array(result)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "\n",
        "def transfer_values_cache(cache_path, model, images=None, image_paths=None):\n",
        "    def fn():\n",
        "        return process_images(fn=model.transfer_values, images=images, image_paths=image_paths)\n",
        "    transfer_values = cache(cache_path=cache_path, fn=fn)\n",
        "\n",
        "    return transfer_values\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    download()\n",
        "    model = Inception()\n",
        "    image_path = os.path.join(data_path, 'cropped_panda.jpg')\n",
        "    pred = model.classify(image_path=image_path)\n",
        "    model.print_scores(pred=pred, k=10)\n",
        "    model.close()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Downloading http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz - 100.00%\n",
            "Download finished. Extracting files.\n",
            "Done.\n",
            "WARNING:tensorflow:From <ipython-input-4-36ea0ff68b9c>:117: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.gfile.GFile.\n",
            "89.11% : giant panda\n",
            " 0.78% : indri\n",
            " 0.30% : lesser panda\n",
            " 0.15% : custard apple\n",
            " 0.12% : earthstar\n",
            " 0.09% : sea urchin\n",
            " 0.05% : forklift\n",
            " 0.05% : digital watch\n",
            " 0.05% : gibbon\n",
            " 0.05% : go-kart\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hk_HzdRH-tEu",
        "outputId": "90303a61-e9b1-483d-9402-ea7bc603452e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "download()\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data already exists.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyJsjImqBQEL"
      },
      "source": [
        "model = Inception()\n",
        "def classify(img_path):\n",
        "    timeStart = int(round(time.time() * 1000))\n",
        "    pred = model.classify(image_path = img_path)\n",
        "    timeStop = int(round(time.time() * 1000))\n",
        "    model.print_scores(pred = pred, k = 5, only_first_name = True)\n",
        "    print(\"Time: \", str(timeStop- timeStart) + \" ms\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiudPevsEysL",
        "outputId": "7c7f1994-1314-43d1-f975-7657b07140fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "classify(img_path = \"/gdrive/My Drive/NesneTanima/2.jpg\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "67.79% : sports car\n",
            "19.24% : racer\n",
            " 2.03% : car wheel\n",
            " 0.60% : grille\n",
            " 0.36% : passenger car\n",
            "Time:  1033 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11yZlF74FK-R"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    }
  ]
}